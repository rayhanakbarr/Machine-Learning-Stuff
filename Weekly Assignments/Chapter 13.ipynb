{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f829efcd",
   "metadata": {},
   "source": [
    "## 1. Pendahuluan\n",
    "\n",
    "### Mengapa Data Pipeline Penting?\n",
    "\n",
    "Deep Learning membutuhkan **data dalam jumlah besar**. Tantangannya:\n",
    "- Data tidak muat di memory\n",
    "- Loading data bisa menjadi bottleneck\n",
    "- Preprocessing harus efisien\n",
    "- GPU idle saat menunggu data\n",
    "\n",
    "### Solusi TensorFlow:\n",
    "\n",
    "1. **tf.data API**: Pipeline yang efisien untuk loading dan preprocessing\n",
    "2. **TFRecord**: Format biner yang optimal untuk large datasets\n",
    "3. **Keras Preprocessing Layers**: Preprocessing terintegrasi dalam model\n",
    "4. **TensorFlow Datasets (TFDS)**: Library dataset siap pakai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82abbf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion 15\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6e219",
   "metadata": {},
   "source": [
    "## 2. The tf.data API\n",
    "\n",
    "`tf.data.Dataset` adalah abstraksi untuk sequence of elements yang bisa di-iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364361b",
   "metadata": {},
   "source": [
    "### 2.1 Membuat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7af8a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dari tensor:\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Dari tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Dataset dari tensor:\")\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1f3deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dengan features dan labels:\n",
      "Features: [1 2], Label: 0\n",
      "Features: [3 4], Label: 1\n",
      "Features: [5 6], Label: 0\n",
      "Features: [7 8], Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Dari multiple tensors (features dan labels)\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "print(\"Dataset dengan features dan labels:\")\n",
    "for features, label in dataset:\n",
    "    print(f\"Features: {features.numpy()}, Label: {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ae4ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dari dictionary:\n",
      "{'feature1': <tf.Tensor: shape=(), dtype=int32, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=int32, numpy=10>, 'label': <tf.Tensor: shape=(), dtype=int32, numpy=0>}\n",
      "{'feature1': <tf.Tensor: shape=(), dtype=int32, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=int32, numpy=20>, 'label': <tf.Tensor: shape=(), dtype=int32, numpy=1>}\n",
      "{'feature1': <tf.Tensor: shape=(), dtype=int32, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=int32, numpy=30>, 'label': <tf.Tensor: shape=(), dtype=int32, numpy=0>}\n",
      "{'feature1': <tf.Tensor: shape=(), dtype=int32, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=int32, numpy=40>, 'label': <tf.Tensor: shape=(), dtype=int32, numpy=1>}\n"
     ]
    }
   ],
   "source": [
    "# Dari dictionary (untuk named features)\n",
    "data_dict = {\n",
    "    'feature1': [1, 2, 3, 4],\n",
    "    'feature2': [10, 20, 30, 40],\n",
    "    'label': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_dict)\n",
    "\n",
    "print(\"Dataset dari dictionary:\")\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9708a221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range dataset: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n"
     ]
    }
   ],
   "source": [
    "# Dataset dengan range\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "print(f\"Range dataset: {list(dataset.as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3a580",
   "metadata": {},
   "source": [
    "### 2.2 Chaining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77ca613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chained transformations:\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n",
      "[0 1 2 3 4]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Method chaining\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "dataset = dataset.repeat(2)      # Repeat 2x\n",
    "dataset = dataset.batch(5)       # Batch size 5\n",
    "\n",
    "print(\"Chained transformations:\")\n",
    "for batch in dataset:\n",
    "    print(batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c4349",
   "metadata": {},
   "source": [
    "## 3. Data Transformations\n",
    "\n",
    "Transformasi umum pada dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450651c7",
   "metadata": {},
   "source": [
    "### 3.1 Map Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10d3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared: [np.int64(0), np.int64(1), np.int64(4), np.int64(9), np.int64(16), np.int64(25), np.int64(36), np.int64(49), np.int64(64), np.int64(81)]\n"
     ]
    }
   ],
   "source": [
    "# Map: apply function ke setiap element\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Square setiap element\n",
    "dataset = dataset.map(lambda x: x ** 2)\n",
    "\n",
    "print(f\"Squared: {list(dataset.as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27215178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing:\n",
      "Features: [0.1 0.2], Label: [1. 0.]\n",
      "Features: [0.3 0.4], Label: [0. 1.]\n",
      "Features: [0.5 0.6], Label: [1. 0.]\n",
      "Features: [0.7 0.8], Label: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Map dengan function yang lebih complex\n",
    "def preprocess(features, label):\n",
    "    # Normalize features\n",
    "    features = tf.cast(features, tf.float32) / 10.0\n",
    "    # One-hot encode label\n",
    "    label = tf.one_hot(label, depth=2)\n",
    "    return features, label\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "for features, label in dataset:\n",
    "    print(f\"Features: {features.numpy()}, Label: {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd277102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel map selesai! Dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Parallel map untuk speedup\n",
    "dataset = tf.data.Dataset.range(1000)\n",
    "\n",
    "# num_parallel_calls untuk parallelism\n",
    "dataset = dataset.map(\n",
    "    lambda x: x ** 2,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE  # Otomatis determine parallelism\n",
    ")\n",
    "\n",
    "print(f\"Parallel map selesai! Dataset size: {len(list(dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2914c",
   "metadata": {},
   "source": [
    "### 3.2 Filter Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601008fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even numbers: [np.int64(0), np.int64(2), np.int64(4), np.int64(6), np.int64(8), np.int64(10), np.int64(12), np.int64(14), np.int64(16), np.int64(18)]\n"
     ]
    }
   ],
   "source": [
    "# Filter: keep elements yang memenuhi kondisi\n",
    "dataset = tf.data.Dataset.range(20)\n",
    "\n",
    "# Keep only even numbers\n",
    "dataset = dataset.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(f\"Even numbers: {list(dataset.as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5dcae",
   "metadata": {},
   "source": [
    "### 3.3 Take dan Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce5e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take 3: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Skip 7: [np.int64(7), np.int64(8), np.int64(9)]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Take: ambil n elements pertama\n",
    "print(f\"Take 3: {list(dataset.take(3).as_numpy_iterator())}\")\n",
    "\n",
    "# Skip: skip n elements pertama\n",
    "print(f\"Skip 7: {list(dataset.skip(7).as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053f7cd",
   "metadata": {},
   "source": [
    "### 3.4 Flat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6adeaed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened: [np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)]\n"
     ]
    }
   ],
   "source": [
    "# Flat map: map lalu flatten\n",
    "# Use uniform-length arrays for from_tensor_slices\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten nested structure\n",
    "dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "print(f\"Flattened: {list(dataset.as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fc51b",
   "metadata": {},
   "source": [
    "## 4. Shuffling, Batching, dan Prefetching\n",
    "\n",
    "Tiga operasi penting untuk training yang efisien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c4c33",
   "metadata": {},
   "source": [
    "### 4.1 Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15319363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled (buffer=5): [np.int64(0), np.int64(1), np.int64(6), np.int64(5), np.int64(7), np.int64(3), np.int64(4), np.int64(8), np.int64(2), np.int64(9)]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle dataset\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# buffer_size: jumlah elements untuk random sampling\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42)\n",
    "\n",
    "print(f\"Shuffled (buffer=5): {list(dataset.as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af58846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size comparison:\n",
      "  Buffer 1: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n",
      "  Buffer 3: [np.int64(1), np.int64(3), np.int64(0), np.int64(4), np.int64(2), np.int64(5), np.int64(6), np.int64(8), np.int64(7), np.int64(9)]\n",
      "  Buffer 10: [np.int64(5), np.int64(3), np.int64(7), np.int64(1), np.int64(4), np.int64(0), np.int64(2), np.int64(8), np.int64(6), np.int64(9)]\n"
     ]
    }
   ],
   "source": [
    "# Efek buffer size\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "print(\"Buffer size comparison:\")\n",
    "for buffer_size in [1, 3, 10]:\n",
    "    shuffled = dataset.shuffle(buffer_size=buffer_size, seed=42)\n",
    "    print(f\"  Buffer {buffer_size}: {list(shuffled.as_numpy_iterator())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d637cf",
   "metadata": {},
   "source": [
    "### 4.2 Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb8c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched (size=4):\n",
      "  [0 1 2 3]\n",
      "  [4 5 6 7]\n",
      "  [ 8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "# Batching\n",
    "dataset = tf.data.Dataset.range(12)\n",
    "\n",
    "# Batch size 4\n",
    "batched = dataset.batch(4)\n",
    "\n",
    "print(\"Batched (size=4):\")\n",
    "for batch in batched:\n",
    "    print(f\"  {batch.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e51dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched with drop_remainder=True:\n",
      "  [0 1 2 3] (size: 4)\n",
      "  [4 5 6 7] (size: 4)\n"
     ]
    }
   ],
   "source": [
    "# Drop remainder (untuk consistent batch size)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "batched = dataset.batch(4, drop_remainder=True)\n",
    "\n",
    "print(\"Batched with drop_remainder=True:\")\n",
    "for batch in batched:\n",
    "    print(f\"  {batch.numpy()} (size: {len(batch)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd53dd",
   "metadata": {},
   "source": [
    "### 4.3 Prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "125d261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline dengan prefetch siap!\n",
      "Number of batches: 32\n"
     ]
    }
   ],
   "source": [
    "# Prefetch: prepare next batch while training on current batch\n",
    "dataset = tf.data.Dataset.range(1000)\n",
    "dataset = dataset.shuffle(100)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Otomatis buffer size\n",
    "\n",
    "print(\"Pipeline dengan prefetch siap!\")\n",
    "print(f\"Number of batches: {len(list(dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86002a94",
   "metadata": {},
   "source": [
    "### 4.4 Optimal Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a58dbf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Data Pipeline:\n",
      "\n",
      "    ┌─────────────┐\n",
      "    │   Dataset   │  <- Raw data\n",
      "    └──────┬──────┘\n",
      "           │\n",
      "    ┌──────▼──────┐\n",
      "    │   Shuffle   │  <- Randomize order (large buffer)\n",
      "    └──────┬──────┘\n",
      "           │\n",
      "    ┌──────▼──────┐\n",
      "    │    Map      │  <- Preprocessing (parallel)\n",
      "    └──────┬──────┘\n",
      "           │\n",
      "    ┌──────▼──────┐\n",
      "    │   Batch     │  <- Create batches\n",
      "    └──────┬──────┘\n",
      "           │\n",
      "    ┌──────▼──────┐\n",
      "    │  Prefetch   │  <- Overlap data loading & training\n",
      "    └──────┬──────┘\n",
      "           │\n",
      "           ▼\n",
      "       Training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualisasi Pipeline\n",
    "print(\"\"\"\n",
    "Optimal Data Pipeline:\n",
    "\n",
    "    ┌─────────────┐\n",
    "    │   Dataset   │  <- Raw data\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "    ┌──────▼──────┐\n",
    "    │   Shuffle   │  <- Randomize order (large buffer)\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "    ┌──────▼──────┐\n",
    "    │    Map      │  <- Preprocessing (parallel)\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "    ┌──────▼──────┐\n",
    "    │   Batch     │  <- Create batches\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "    ┌──────▼──────┐\n",
    "    │  Prefetch   │  <- Overlap data loading & training\n",
    "    └──────┬──────┘\n",
    "           │\n",
    "           ▼\n",
    "       Training\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f82802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal pipeline created! Batches: 32\n"
     ]
    }
   ],
   "source": [
    "# Complete optimal pipeline\n",
    "def create_optimal_pipeline(X, y, batch_size=32, shuffle_buffer=1000):\n",
    "    \"\"\"\n",
    "    Membuat data pipeline yang optimal untuk training\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    # Shuffle\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
    "    \n",
    "    # Map (preprocessing) dengan parallelism\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (tf.cast(x, tf.float32), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Example\n",
    "X = np.random.randn(1000, 10)\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "train_dataset = create_optimal_pipeline(X, y, batch_size=32)\n",
    "print(f\"Optimal pipeline created! Batches: {len(list(train_dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36610fb9",
   "metadata": {},
   "source": [
    "## 5. Reading Data from Files\n",
    "\n",
    "Membaca data dari berbagai format file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b8eeb",
   "metadata": {},
   "source": [
    "### 5.1 CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f3a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file created!\n"
     ]
    }
   ],
   "source": [
    "# Buat sample CSV file\n",
    "csv_data = \"\"\"feature1,feature2,label\n",
    "1.0,2.0,0\n",
    "3.0,4.0,1\n",
    "5.0,6.0,0\n",
    "7.0,8.0,1\n",
    "9.0,10.0,0\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('sample_data.csv', 'w') as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "print(\"Sample CSV file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70117642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data loaded:\n",
      "  Features: [1. 2.], Label: 0\n",
      "  Features: [3. 4.], Label: 1\n",
      "  Features: [5. 6.], Label: 0\n",
      "  Features: [7. 8.], Label: 1\n",
      "  Features: [ 9. 10.], Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Read CSV dengan tf.data\n",
    "def parse_csv_line(line):\n",
    "    # Define column types\n",
    "    defaults = [0.0, 0.0, 0]  # feature1, feature2, label\n",
    "    parsed = tf.io.decode_csv(line, defaults)\n",
    "    features = tf.stack(parsed[:-1])  # All except last\n",
    "    label = parsed[-1]  # Last column\n",
    "    return features, label\n",
    "\n",
    "# Create dataset from CSV\n",
    "dataset = tf.data.TextLineDataset('sample_data.csv')\n",
    "dataset = dataset.skip(1)  # Skip header\n",
    "dataset = dataset.map(parse_csv_line)\n",
    "\n",
    "print(\"CSV data loaded:\")\n",
    "for features, label in dataset:\n",
    "    print(f\"  Features: {features.numpy()}, Label: {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad271472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['.\\\\data_part0.csv', '.\\\\data_part1.csv', '.\\\\data_part2.csv']\n",
      "\n",
      "Interleaved data:\n",
      "  10,1\n",
      "  0,0\n",
      "  20,2\n",
      "  11,1\n",
      "  1,0\n",
      "  21,2\n"
     ]
    }
   ],
   "source": [
    "# Multiple CSV files\n",
    "# Buat beberapa file\n",
    "for i in range(3):\n",
    "    with open(f'data_part{i}.csv', 'w') as f:\n",
    "        f.write(f\"feature,label\\n{i*10},{i}\\n{i*10+1},{i}\")\n",
    "\n",
    "# Read multiple files\n",
    "file_pattern = 'data_part*.csv'\n",
    "filenames = tf.io.gfile.glob(file_pattern)\n",
    "print(f\"Found files: {filenames}\")\n",
    "\n",
    "# Interleave untuk parallel reading\n",
    "dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=3,  # Number of files to read in parallel\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"\\nInterleaved data:\")\n",
    "for line in dataset:\n",
    "    print(f\"  {line.numpy().decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279745f",
   "metadata": {},
   "source": [
    "### 5.2 Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0954b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loading function defined!\n"
     ]
    }
   ],
   "source": [
    "# Membaca image files (contoh)\n",
    "def load_and_preprocess_image(filepath, label):\n",
    "    # Read file\n",
    "    image = tf.io.read_file(filepath)\n",
    "    # Decode (supports JPEG, PNG, etc)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    # Resize\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    # Normalize\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Example usage:\n",
    "# filepaths = ['path/to/image1.jpg', 'path/to/image2.jpg']\n",
    "# labels = [0, 1]\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "# dataset = dataset.map(load_and_preprocess_image)\n",
    "\n",
    "print(\"Image loading function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a723d82",
   "metadata": {},
   "source": [
    "## 6. TFRecord Format\n",
    "\n",
    "**TFRecord** adalah format biner TensorFlow yang optimal untuk large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e881716",
   "metadata": {},
   "source": [
    "### 6.1 Membuat TFRecord File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeb267e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions untuk TFRecord\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_array_feature(value):\n",
    "    \"\"\"Returns a float_list from a numpy array.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "print(\"TFRecord helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8b8867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord file created: sample_data.tfrecord\n",
      "File size: 9200 bytes\n"
     ]
    }
   ],
   "source": [
    "# Membuat TFRecord file\n",
    "def serialize_example(features, label):\n",
    "    \"\"\"Serialize satu example ke TFRecord format\"\"\"\n",
    "    feature_dict = {\n",
    "        'features': _float_array_feature(features),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    example_proto = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature_dict)\n",
    "    )\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 10).astype(np.float32)\n",
    "y = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "# Write to TFRecord\n",
    "tfrecord_file = 'sample_data.tfrecord'\n",
    "with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
    "    for features, label in zip(X, y):\n",
    "        example = serialize_example(features, label)\n",
    "        writer.write(example)\n",
    "\n",
    "print(f\"TFRecord file created: {tfrecord_file}\")\n",
    "print(f\"File size: {os.path.getsize(tfrecord_file)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3353ae",
   "metadata": {},
   "source": [
    "### 6.2 Membaca TFRecord File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c375c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord data loaded:\n",
      "  Features shape: (10,), Label: 1\n",
      "  Features shape: (10,), Label: 1\n",
      "  Features shape: (10,), Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Parse function untuk TFRecord\n",
    "def parse_tfrecord(serialized_example):\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature([10], tf.float32),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    return example['features'], example['label']\n",
    "\n",
    "# Read TFRecord\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "dataset = dataset.map(parse_tfrecord)\n",
    "\n",
    "# Show first few examples\n",
    "print(\"TFRecord data loaded:\")\n",
    "for features, label in dataset.take(3):\n",
    "    print(f\"  Features shape: {features.shape}, Label: {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45218e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed TFRecord: 553 bytes\n"
     ]
    }
   ],
   "source": [
    "# Compressed TFRecord\n",
    "compressed_file = 'sample_data.tfrecord.gz'\n",
    "\n",
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter(compressed_file, options=options) as writer:\n",
    "    for features, label in zip(X[:10], y[:10]):\n",
    "        example = serialize_example(features, label)\n",
    "        writer.write(example)\n",
    "\n",
    "print(f\"Compressed TFRecord: {os.path.getsize(compressed_file)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64690643",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Features\n",
    "\n",
    "Preprocessing berbagai tipe features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18488a08",
   "metadata": {},
   "source": [
    "### 7.1 Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7caa8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [ 1.  5. 10. 15. 20.]\n",
      "Standardized: [-1.3541131  -0.7653683  -0.02943721  0.70649385  1.4424249 ]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "def standardize(features, mean, std):\n",
    "    return (features - mean) / std\n",
    "\n",
    "# Normalization (min-max)\n",
    "def normalize(features, min_val, max_val):\n",
    "    return (features - min_val) / (max_val - min_val)\n",
    "\n",
    "# Example\n",
    "data = tf.constant([1.0, 5.0, 10.0, 15.0, 20.0])\n",
    "\n",
    "mean = tf.reduce_mean(data)\n",
    "std = tf.math.reduce_std(data)\n",
    "standardized = standardize(data, mean, std)\n",
    "\n",
    "print(f\"Original: {data.numpy()}\")\n",
    "print(f\"Standardized: {standardized.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a3cde",
   "metadata": {},
   "source": [
    "### 7.2 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b6a7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "categories = tf.constant([0, 1, 2, 1, 0])\n",
    "one_hot = tf.one_hot(categories, depth=3)\n",
    "\n",
    "print(\"One-hot encoding:\")\n",
    "print(one_hot.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03342133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: [b'dog' b'cat' b'bird' b'fish']\n",
      "Indices: [ 1  0  2 -1]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary lookup (string to integer)\n",
    "vocab = ['cat', 'dog', 'bird']\n",
    "lookup_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=vocab,\n",
    "        values=tf.range(len(vocab))\n",
    "    ),\n",
    "    default_value=-1\n",
    ")\n",
    "\n",
    "# Lookup\n",
    "words = tf.constant(['dog', 'cat', 'bird', 'fish'])\n",
    "indices = lookup_table.lookup(words)\n",
    "\n",
    "print(f\"Words: {words.numpy()}\")\n",
    "print(f\"Indices: {indices.numpy()}\")  # 'fish' akan jadi -1 (unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4b9ec",
   "metadata": {},
   "source": [
    "### 7.3 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f152026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 3)\n",
      "Embedding shape: (2, 3, 16)\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# Example: word indices -> embeddings\n",
    "word_indices = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "embeddings = embedding_layer(word_indices)\n",
    "\n",
    "print(f\"Input shape: {word_indices.shape}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b666d07",
   "metadata": {},
   "source": [
    "## 8. Keras Preprocessing Layers\n",
    "\n",
    "Keras menyediakan preprocessing layers yang dapat dimasukkan ke dalam model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb760d",
   "metadata": {},
   "source": [
    "### 8.1 Numerical Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11fde6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [ 1.  5. 10. 15. 20.]\n",
      "Normalized: [-1.354113   -0.7653682  -0.02943721  0.7064938   1.4424248 ]\n",
      "Mean: [[10.2]], Variance: [[46.16]]\n"
     ]
    }
   ],
   "source": [
    "# Normalization layer\n",
    "normalizer = keras.layers.Normalization()\n",
    "\n",
    "# Adapt to data (compute mean dan variance)\n",
    "data = np.array([[1.0], [5.0], [10.0], [15.0], [20.0]])\n",
    "normalizer.adapt(data)\n",
    "\n",
    "# Transform\n",
    "normalized = normalizer(data)\n",
    "print(f\"Original: {data.flatten()}\")\n",
    "print(f\"Normalized: {normalized.numpy().flatten()}\")\n",
    "print(f\"Mean: {normalizer.mean.numpy()}, Variance: {normalizer.variance.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "759e5330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [-5.  2.  7. 12. 20.]\n",
      "Discretized: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Discretization (binning)\n",
    "discretizer = keras.layers.Discretization(bin_boundaries=[0, 5, 10, 15])\n",
    "\n",
    "data = tf.constant([[-5.0], [2.0], [7.0], [12.0], [20.0]])\n",
    "discretized = discretizer(data)\n",
    "\n",
    "print(f\"Original: {data.numpy().flatten()}\")\n",
    "print(f\"Discretized: {discretized.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57949f5",
   "metadata": {},
   "source": [
    "### 8.2 Categorical Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c73bedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: [b'dog' b'cat' b'bird' b'fish']\n",
      "Indices: [2 1 3 0]\n"
     ]
    }
   ],
   "source": [
    "# StringLookup - string ke integer\n",
    "string_lookup = keras.layers.StringLookup(\n",
    "    vocabulary=['cat', 'dog', 'bird'],\n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "words = tf.constant(['dog', 'cat', 'bird', 'fish'])\n",
    "indices = string_lookup(words)\n",
    "\n",
    "print(f\"Words: {words.numpy()}\")\n",
    "print(f\"Indices: {indices.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "614da4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# CategoryEncoding - integer ke one-hot atau multi-hot\n",
    "encoder = keras.layers.CategoryEncoding(\n",
    "    num_tokens=5,\n",
    "    output_mode='one_hot'\n",
    ")\n",
    "\n",
    "categories = tf.constant([0, 1, 2, 3, 4])\n",
    "encoded = encoder(categories)\n",
    "\n",
    "print(\"One-hot encoded:\")\n",
    "print(encoded.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38bb8fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 2 3 4]\n",
      "Encoded: [1 2 3 0]\n",
      "Vocabulary: [-1, np.int64(1), np.int64(2), np.int64(3)]\n"
     ]
    }
   ],
   "source": [
    "# IntegerLookup dengan vocabulary dari data\n",
    "integer_lookup = keras.layers.IntegerLookup()\n",
    "integer_lookup.adapt([1, 2, 3, 1, 2, 1])  # Learn vocabulary\n",
    "\n",
    "data = tf.constant([1, 2, 3, 4])  # 4 is unknown\n",
    "result = integer_lookup(data)\n",
    "\n",
    "print(f\"Input: {data.numpy()}\")\n",
    "print(f\"Encoded: {result.numpy()}\")\n",
    "print(f\"Vocabulary: {integer_lookup.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ab729",
   "metadata": {},
   "source": [
    "### 8.3 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da9e63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (first 20):\n",
      "['', '[UNK]', np.str_('learning'), np.str_('is'), np.str_('deep'), np.str_('the'), np.str_('tensorflow'), np.str_('neural'), np.str_('networks'), np.str_('makes'), np.str_('keras'), np.str_('great'), np.str_('future'), np.str_('for'), np.str_('easy')]\n",
      "\n",
      "Vectorized shape: (3, 10)\n",
      "First text vectorized: [ 6  3 11 13  4  2  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization layer\n",
    "text_vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=100,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10\n",
    ")\n",
    "\n",
    "# Adapt to text data\n",
    "texts = [\n",
    "    \"TensorFlow is great for deep learning\",\n",
    "    \"Keras makes neural networks easy\",\n",
    "    \"Deep learning is the future\"\n",
    "]\n",
    "text_vectorizer.adapt(texts)\n",
    "\n",
    "# Vectorize\n",
    "vectorized = text_vectorizer(texts)\n",
    "\n",
    "print(\"Vocabulary (first 20):\")\n",
    "print(text_vectorizer.get_vocabulary()[:20])\n",
    "print(f\"\\nVectorized shape: {vectorized.shape}\")\n",
    "print(f\"First text vectorized: {vectorized[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889a62f",
   "metadata": {},
   "source": [
    "### 8.4 Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84af6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image preprocessing layers:\n",
      "- Resizing: Resize images to fixed size\n",
      "- Rescaling: Normalize pixel values\n",
      "- RandomFlip: Random horizontal/vertical flip\n",
      "- RandomRotation: Random rotation\n",
      "- RandomZoom: Random zoom in/out\n",
      "- RandomContrast: Random contrast adjustment\n"
     ]
    }
   ],
   "source": [
    "# Image preprocessing layers\n",
    "image_preprocess = keras.Sequential([\n",
    "    keras.layers.Resizing(224, 224),\n",
    "    keras.layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "# Data augmentation layers\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal'),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomContrast(0.1)\n",
    "])\n",
    "\n",
    "print(\"Image preprocessing layers:\")\n",
    "print(\"- Resizing: Resize images to fixed size\")\n",
    "print(\"- Rescaling: Normalize pixel values\")\n",
    "print(\"- RandomFlip: Random horizontal/vertical flip\")\n",
    "print(\"- RandomRotation: Random rotation\")\n",
    "print(\"- RandomZoom: Random zoom in/out\")\n",
    "print(\"- RandomContrast: Random contrast adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7ef79",
   "metadata": {},
   "source": [
    "### 8.5 Model dengan Preprocessing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92366509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.5291 - val_loss: 0.6027\n",
      "Epoch 2/5\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4939 - val_loss: 0.4669\n",
      "Epoch 3/5\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4210 - val_loss: 0.4378\n",
      "Epoch 4/5\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3932 - val_loss: 0.4183\n",
      "Epoch 5/5\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3785 - val_loss: 0.4072\n",
      "\n",
      "Model dengan preprocessing layers selesai training!\n"
     ]
    }
   ],
   "source": [
    "# Model dengan preprocessing layers terintegrasi\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42\n",
    ")\n",
    "\n",
    "# Create normalizer\n",
    "normalizer = keras.layers.Normalization()\n",
    "normalizer.adapt(X_train)\n",
    "\n",
    "# Model dengan preprocessing di dalam\n",
    "model = keras.Sequential([\n",
    "    normalizer,  # Preprocessing layer\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Training - tidak perlu preprocessing manual!\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel dengan preprocessing layers selesai training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77795d",
   "metadata": {},
   "source": [
    "## 9. TensorFlow Datasets (TFDS)\n",
    "\n",
    "Library untuk mengakses dataset siap pakai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8e806fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.9.9)\n",
      "Requirement already satisfied: absl-py in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (2.3.1)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (4.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (2.2.0)\n",
      "Requirement already satisfied: promise in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (6.33.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (6.1.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (22.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (2.32.5)\n",
      "Requirement already satisfied: simple_parsing in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (1.17.3)\n",
      "Requirement already satisfied: termcolor in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (3.3.0)\n",
      "Requirement already satisfied: toml in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-datasets) (2.0.1)\n",
      "Requirement already satisfied: einops in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.12.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.15.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.11.12)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from dm-tree->tensorflow-datasets) (25.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from simple_parsing->tensorflow-datasets) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.72.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp pavilion 15\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->tensorflow-datasets) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "TFDS version: 4.9.9\n"
     ]
    }
   ],
   "source": [
    "# Install TFDS jika belum\n",
    "%pip install tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "print(f\"TFDS version: {tfds.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0afce183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular datasets in TFDS:\n",
      "  - mnist\n",
      "  - fashion_mnist\n",
      "  - cifar10\n",
      "  - cifar100\n",
      "  - imdb_reviews\n",
      "  - coco\n",
      "  - imagenet2012\n"
     ]
    }
   ],
   "source": [
    "# List available datasets\n",
    "try:\n",
    "    import tensorflow_datasets as tfds\n",
    "    \n",
    "    # Beberapa dataset populer\n",
    "    popular_datasets = [\n",
    "        'mnist',\n",
    "        'fashion_mnist',\n",
    "        'cifar10',\n",
    "        'cifar100',\n",
    "        'imdb_reviews',\n",
    "        'coco',\n",
    "        'imagenet2012'\n",
    "    ]\n",
    "    \n",
    "    print(\"Popular datasets in TFDS:\")\n",
    "    for ds in popular_datasets:\n",
    "        print(f\"  - {ds}\")\n",
    "except:\n",
    "    print(\"TFDS not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ab49a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\HP Pavilion 15\\tensorflow_datasets\\mnist\\3.0.1 has no dataset_info.json\n",
      "C:\\Users\\HP Pavilion 15\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\HP Pavilion 15\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 140.08 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 94.86 url/s] \n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 71.12 url/s]\n",
      "Dl Completed...:  50%|█████     | 1/2 [00:00<00:00, 56.16 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 97.39 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 84.20 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 75.88 url/s]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 65.24 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 88.31 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 81.36 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 75.42 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00, 68.32 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 85.70 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 80.55 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 75.77 url/s]\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Dl Size...: 100%|██████████| 11594722/11594722 [00:00<00:00, 208459772.83 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 68.94 url/s]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\HP Pavilion 15\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "Dataset: mnist\n",
      "Description: The MNIST database of handwritten digits....\n",
      "Features: FeaturesDict({\n",
      "    'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "    'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "})\n",
      "Total examples: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load dataset dengan TFDS\n",
    "try:\n",
    "    import tensorflow_datasets as tfds\n",
    "    \n",
    "    # Load MNIST\n",
    "    dataset, info = tfds.load(\n",
    "        'mnist',\n",
    "        split='train',\n",
    "        with_info=True,\n",
    "        as_supervised=True  # Returns (image, label) tuple\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {info.name}\")\n",
    "    print(f\"Description: {info.description[:100]}...\")\n",
    "    print(f\"Features: {info.features}\")\n",
    "    print(f\"Total examples: {info.splits['train'].num_examples}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load TFDS: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2a23663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFDS pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "# Contoh pipeline dengan TFDS\n",
    "try:\n",
    "    import tensorflow_datasets as tfds\n",
    "    \n",
    "    def preprocess_mnist(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image, label\n",
    "    \n",
    "    # Load dan preprocess\n",
    "    train_ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "    train_ds = train_ds.map(preprocess_mnist)\n",
    "    train_ds = train_ds.shuffle(10000)\n",
    "    train_ds = train_ds.batch(32)\n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(\"TFDS pipeline ready!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"TFDS example skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0897c7",
   "metadata": {},
   "source": [
    "## 10. Kesimpulan\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **tf.data API** menyediakan pipeline yang efisien:\n",
    "   - `from_tensor_slices()`: Buat dataset dari tensors\n",
    "   - `map()`: Apply transformations\n",
    "   - `filter()`: Filter elements\n",
    "   - `batch()`: Create batches\n",
    "   - `shuffle()`: Randomize order\n",
    "   - `prefetch()`: Overlap loading dan training\n",
    "\n",
    "2. **Optimal Pipeline Pattern**:\n",
    "   ```python\n",
    "   dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "   dataset = dataset.shuffle(buffer_size)\n",
    "   dataset = dataset.map(preprocess_fn, num_parallel_calls=AUTOTUNE)\n",
    "   dataset = dataset.batch(batch_size)\n",
    "   dataset = dataset.prefetch(AUTOTUNE)\n",
    "   ```\n",
    "\n",
    "3. **TFRecord** untuk large datasets:\n",
    "   - Binary format yang efisien\n",
    "   - Support compression\n",
    "   - Optimal untuk distributed training\n",
    "\n",
    "4. **Keras Preprocessing Layers**:\n",
    "   - `Normalization`: Standardize numerical features\n",
    "   - `StringLookup`: String to integer encoding\n",
    "   - `CategoryEncoding`: One-hot/multi-hot encoding\n",
    "   - `TextVectorization`: Text to integers\n",
    "   - `Resizing`, `Rescaling`: Image preprocessing\n",
    "   - `RandomFlip`, `RandomRotation`: Data augmentation\n",
    "\n",
    "5. **TensorFlow Datasets (TFDS)**:\n",
    "   - Library dataset siap pakai\n",
    "   - Easy loading dengan `tfds.load()`\n",
    "   - Built-in preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efc61531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary files cleaned up!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup temporary files\n",
    "import os\n",
    "\n",
    "temp_files = [\n",
    "    'sample_data.csv',\n",
    "    'sample_data.tfrecord',\n",
    "    'sample_data.tfrecord.gz',\n",
    "    'data_part0.csv',\n",
    "    'data_part1.csv',\n",
    "    'data_part2.csv'\n",
    "]\n",
    "\n",
    "for f in temp_files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "print(\"Temporary files cleaned up!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
