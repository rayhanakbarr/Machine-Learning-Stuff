{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d044e5e",
   "metadata": {},
   "source": [
    "## 1. Pendahuluan\n",
    "\n",
    "**Reinforcement Learning (RL)** adalah paradigma machine learning dimana agent belajar melalui interaksi dengan environment.\n",
    "\n",
    "### Perbedaan dengan ML Lainnya:\n",
    "\n",
    "| Paradigm | Data | Learning Signal |\n",
    "|----------|------|----------------|\n",
    "| **Supervised** | Labeled (x, y) pairs | Correct answer |\n",
    "| **Unsupervised** | Unlabeled data | Find patterns |\n",
    "| **Reinforcement** | Experience (s, a, r, s') | Reward signal |\n",
    "\n",
    "### RL Applications:\n",
    "- ğŸ® Game playing (AlphaGo, Atari, Chess)\n",
    "- ğŸ¤– Robotics (manipulation, locomotion)\n",
    "- ğŸš— Autonomous vehicles\n",
    "- ğŸ“ˆ Trading & finance\n",
    "- ğŸ­ Resource management\n",
    "- ğŸ’Š Drug discovery\n",
    "- ğŸ—£ï¸ Dialogue systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1fb7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion 15\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dff260",
   "metadata": {},
   "source": [
    "## 2. Komponen Reinforcement Learning\n",
    "\n",
    "### 2.1 Core Components\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                                             â”‚\n",
    "    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "    â”‚    â”‚  Agent  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Environment â”‚     â”‚\n",
    "    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "    â”‚         â”‚                     â”‚            â”‚\n",
    "    â”‚         â”‚    Action (a)       â”‚            â”‚\n",
    "    â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚            â”‚\n",
    "    â”‚         â”‚                     â”‚            â”‚\n",
    "    â”‚         â”‚  State (s), Reward (r)           â”‚\n",
    "    â”‚         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚\n",
    "    â”‚                                             â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Agent** | Learner/decision maker |\n",
    "| **Environment** | Everything outside agent |\n",
    "| **State (s)** | Current situation |\n",
    "| **Action (a)** | What agent does |\n",
    "| **Reward (r)** | Feedback signal |\n",
    "| **Policy (Ï€)** | Agent's behavior/strategy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de281baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reinforcement Learning Loop:\n",
      "\n",
      "At each timestep t:\n",
      "1. Agent observes state sâ‚œ\n",
      "2. Agent takes action aâ‚œ based on policy Ï€(sâ‚œ)\n",
      "3. Environment transitions to new state sâ‚œâ‚Šâ‚\n",
      "4. Environment gives reward râ‚œâ‚Šâ‚\n",
      "5. Agent learns from (sâ‚œ, aâ‚œ, râ‚œâ‚Šâ‚, sâ‚œâ‚Šâ‚)\n",
      "6. Repeat...\n",
      "\n",
      "Goal: Maximize cumulative reward over time!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RL Loop visualization\n",
    "print(\"\"\"\n",
    "Reinforcement Learning Loop:\n",
    "\n",
    "At each timestep t:\n",
    "1. Agent observes state sâ‚œ\n",
    "2. Agent takes action aâ‚œ based on policy Ï€(sâ‚œ)\n",
    "3. Environment transitions to new state sâ‚œâ‚Šâ‚\n",
    "4. Environment gives reward râ‚œâ‚Šâ‚\n",
    "5. Agent learns from (sâ‚œ, aâ‚œ, râ‚œâ‚Šâ‚, sâ‚œâ‚Šâ‚)\n",
    "6. Repeat...\n",
    "\n",
    "Goal: Maximize cumulative reward over time!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4558513",
   "metadata": {},
   "source": [
    "### 2.2 Simple Environment Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f018e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      "A . . .\n",
      ". X . .\n",
      ". . . .\n",
      ". . . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple GridWorld Environment\n",
    "class SimpleGridWorld:\n",
    "    \"\"\"\n",
    "    4x4 GridWorld:\n",
    "    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "    â”‚ S â”‚   â”‚   â”‚   â”‚\n",
    "    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "    â”‚   â”‚ X â”‚   â”‚   â”‚  X = obstacle\n",
    "    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "    â”‚   â”‚   â”‚   â”‚   â”‚\n",
    "    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "    â”‚   â”‚   â”‚   â”‚ G â”‚  G = goal (+1 reward)\n",
    "    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.grid_size = 4\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (3, 3)\n",
    "        self.obstacle = (1, 1)\n",
    "        self.state = self.start\n",
    "        \n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (0, 1),   # right\n",
    "            2: (1, 0),   # down\n",
    "            3: (0, -1)   # left\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action, return (new_state, reward, done)\"\"\"\n",
    "        # Calculate new position\n",
    "        dy, dx = self.actions[action]\n",
    "        new_row = max(0, min(self.grid_size - 1, self.state[0] + dy))\n",
    "        new_col = max(0, min(self.grid_size - 1, self.state[1] + dx))\n",
    "        new_state = (new_row, new_col)\n",
    "        \n",
    "        # Check obstacle\n",
    "        if new_state == self.obstacle:\n",
    "            new_state = self.state  # Stay in place\n",
    "        \n",
    "        self.state = new_state\n",
    "        \n",
    "        # Reward\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 1.0, True  # Goal reached!\n",
    "        else:\n",
    "            return self.state, -0.01, False  # Small penalty per step\n",
    "    \n",
    "    def render(self):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        grid[self.obstacle] = 'X'\n",
    "        grid[self.goal] = 'G'\n",
    "        grid[self.state] = 'A'\n",
    "        print('\\n'.join([' '.join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "# Test environment\n",
    "env = SimpleGridWorld()\n",
    "state = env.reset()\n",
    "print(\"Initial state:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b134dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Agent Playing:\n",
      "Total reward: -0.20\n",
      ". . . .\n",
      ". X . .\n",
      ". . . .\n",
      "A . . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random agent\n",
    "env = SimpleGridWorld()\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"Random Agent Playing:\")\n",
    "for _ in range(20):\n",
    "    action = np.random.randint(4)  # Random action\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nGoal reached in {steps} steps!\")\n",
    "        break\n",
    "\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b36c3",
   "metadata": {},
   "source": [
    "## 3. Policy dan Value Functions\n",
    "\n",
    "### 3.1 Policy (Ï€)\n",
    "\n",
    "**Policy** adalah mapping dari states ke actions.\n",
    "\n",
    "- **Deterministic Policy**: Ï€(s) = a\n",
    "- **Stochastic Policy**: Ï€(a|s) = P(action=a | state=s)\n",
    "\n",
    "### 3.2 Value Functions\n",
    "\n",
    "**State-Value Function V(s):**\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_0 = s \\right]\n",
    "$$\n",
    "\n",
    "Expected cumulative reward starting from state s, following policy Ï€.\n",
    "\n",
    "**Action-Value Function Q(s, a):**\n",
    "$$\n",
    "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_0 = s, a_0 = a \\right]\n",
    "$$\n",
    "\n",
    "Expected cumulative reward starting from state s, taking action a, then following Ï€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d4a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Functions:\n",
      "\n",
      "V(s) = \"How good is this state?\"\n",
      "       Expected future reward from state s\n",
      "\n",
      "Q(s,a) = \"How good is this action in this state?\"\n",
      "         Expected future reward from state s, taking action a\n",
      "\n",
      "Relationship:\n",
      "  V(s) = max_a Q(s, a)  (for optimal policy)\n",
      "  V(s) = Î£_a Ï€(a|s) Ã— Q(s, a)  (for any policy)\n",
      "\n",
      "Discount Factor (Î³):\n",
      "  - Î³ âˆˆ [0, 1]\n",
      "  - Î³ = 0: Only care about immediate reward\n",
      "  - Î³ = 1: Care equally about all future rewards\n",
      "  - Typical: Î³ = 0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value function concepts\n",
    "print(\"\"\"\n",
    "Value Functions:\n",
    "\n",
    "V(s) = \"How good is this state?\"\n",
    "       Expected future reward from state s\n",
    "\n",
    "Q(s,a) = \"How good is this action in this state?\"\n",
    "         Expected future reward from state s, taking action a\n",
    "\n",
    "Relationship:\n",
    "  V(s) = max_a Q(s, a)  (for optimal policy)\n",
    "  V(s) = Î£_a Ï€(a|s) Ã— Q(s, a)  (for any policy)\n",
    "\n",
    "Discount Factor (Î³):\n",
    "  - Î³ âˆˆ [0, 1]\n",
    "  - Î³ = 0: Only care about immediate reward\n",
    "  - Î³ = 1: Care equally about all future rewards\n",
    "  - Typical: Î³ = 0.99\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc424c",
   "metadata": {},
   "source": [
    "### 3.3 Bellman Equations\n",
    "\n",
    "**Bellman Equation** menghubungkan value function dengan successor states.\n",
    "\n",
    "**Bellman Expectation Equation:**\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^\\pi(s')]\n",
    "$$\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "$$\n",
    "V^*(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} Q^*(s', a')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b63f5",
   "metadata": {},
   "source": [
    "## 4. Markov Decision Process (MDP)\n",
    "\n",
    "**MDP** adalah mathematical framework untuk RL.\n",
    "\n",
    "### MDP Components:\n",
    "- **S**: Set of states\n",
    "- **A**: Set of actions\n",
    "- **P(s'|s, a)**: Transition probability\n",
    "- **R(s, a, s')**: Reward function\n",
    "- **Î³**: Discount factor\n",
    "\n",
    "### Markov Property:\n",
    "$$\n",
    "P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)\n",
    "$$\n",
    "\n",
    "Future depends only on current state, not history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d026cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example MDP: Robot Navigation\n",
      "\n",
      "States (S): {Room A, Room B, Room C, Charging Station}\n",
      "Actions (A): {Move Left, Move Right, Stay, Charge}\n",
      "\n",
      "Transition P(s'|s, a):\n",
      "  - From Room A, Move Right â†’ Room B (90%), Stay in A (10%)\n",
      "  - Probabilistic due to wheel slippage\n",
      "\n",
      "Rewards R(s, a, s'):\n",
      "  - Reach Charging Station: +10\n",
      "  - Each move: -1 (energy cost)\n",
      "  - Stay: 0\n",
      "\n",
      "Goal: Find optimal policy to maximize expected rewards\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MDP example\n",
    "print(\"\"\"\n",
    "Example MDP: Robot Navigation\n",
    "\n",
    "States (S): {Room A, Room B, Room C, Charging Station}\n",
    "Actions (A): {Move Left, Move Right, Stay, Charge}\n",
    "\n",
    "Transition P(s'|s, a):\n",
    "  - From Room A, Move Right â†’ Room B (90%), Stay in A (10%)\n",
    "  - Probabilistic due to wheel slippage\n",
    "\n",
    "Rewards R(s, a, s'):\n",
    "  - Reach Charging Station: +10\n",
    "  - Each move: -1 (energy cost)\n",
    "  - Stay: 0\n",
    "\n",
    "Goal: Find optimal policy to maximize expected rewards\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4b56b",
   "metadata": {},
   "source": [
    "## 5. Q-Learning\n",
    "\n",
    "**Q-Learning** adalah **model-free**, **off-policy** algorithm.\n",
    "\n",
    "### Update Rule:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]\n",
    "$$\n",
    "\n",
    "Dimana:\n",
    "- Î± = learning rate\n",
    "- Î³ = discount factor\n",
    "- $r + \\gamma \\max_{a'} Q(s', a')$ = TD target\n",
    "- $r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$ = TD error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4a9c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Agent created!\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning implementation\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha      # Learning rate\n",
    "        self.gamma = gamma      # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    def state_to_idx(self, state):\n",
    "        \"\"\"Convert (row, col) to single index\"\"\"\n",
    "        return state[0] * 4 + state[1]\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)  # Explore\n",
    "        else:\n",
    "            state_idx = self.state_to_idx(state)\n",
    "            return np.argmax(self.q_table[state_idx])  # Exploit\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-Learning update\"\"\"\n",
    "        state_idx = self.state_to_idx(state)\n",
    "        next_state_idx = self.state_to_idx(next_state)\n",
    "        \n",
    "        # TD target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - self.q_table[state_idx, action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.q_table[state_idx, action] += self.alpha * td_error\n",
    "\n",
    "print(\"Q-Learning Agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1090b11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Average reward (last 100 episodes): 0.95\n"
     ]
    }
   ],
   "source": [
    "# Train Q-Learning agent\n",
    "env = SimpleGridWorld()\n",
    "agent = QLearningAgent(n_states=16, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "\n",
    "n_episodes = 1000\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(100):  # Max steps per episode\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards_history.append(total_reward)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
    "\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Average reward (last 100 episodes): {np.mean(rewards_history[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098a7497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "â†’ â†’ â†“ â†“\n",
      "â†“ X â†“ â†“\n",
      "â†’ â†’ â†’ â†“\n",
      "â†“ â†’ â†‘ G\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_history, alpha=0.3)\n",
    "# Moving average\n",
    "window = 50\n",
    "moving_avg = [np.mean(rewards_history[max(0, i-window):i+1]) for i in range(len(rewards_history))]\n",
    "plt.plot(moving_avg, 'r-', linewidth=2, label=f'{window}-episode moving avg')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning: Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Visualize Q-table as policy\n",
    "action_symbols = ['â†‘', 'â†’', 'â†“', 'â†']\n",
    "policy_grid = np.zeros((4, 4), dtype=object)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        state_idx = i * 4 + j\n",
    "        best_action = np.argmax(agent.q_table[state_idx])\n",
    "        policy_grid[i, j] = action_symbols[best_action]\n",
    "\n",
    "policy_grid[3, 3] = 'G'\n",
    "policy_grid[1, 1] = 'X'\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "for row in policy_grid:\n",
    "    print(' '.join(row))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd98a10",
   "metadata": {},
   "source": [
    "### 5.1 Exploration vs Exploitation\n",
    "\n",
    "**Trade-off:**\n",
    "- **Exploration**: Try new actions to discover better strategies\n",
    "- **Exploitation**: Use known good actions to maximize reward\n",
    "\n",
    "**Strategies:**\n",
    "- **Îµ-greedy**: Random action with probability Îµ\n",
    "- **Softmax/Boltzmann**: P(a) âˆ exp(Q(s,a)/Ï„)\n",
    "- **UCB (Upper Confidence Bound)**: Optimism in face of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3554d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploration Strategies:\n",
      "\n",
      "1. Îµ-greedy:\n",
      "   - With prob Îµ: random action\n",
      "   - With prob 1-Îµ: best action\n",
      "   - Simple and effective\n",
      "\n",
      "2. Softmax/Boltzmann:\n",
      "   P(a|s) = exp(Q(s,a)/Ï„) / Î£ exp(Q(s,a')/Ï„)\n",
      "   - Ï„ high â†’ more exploration\n",
      "   - Ï„ low â†’ more exploitation\n",
      "\n",
      "3. Upper Confidence Bound (UCB):\n",
      "   a = argmax[Q(s,a) + câˆš(ln(t)/N(s,a))]\n",
      "   - Explores less-visited actions\n",
      "\n",
      "4. Decaying Îµ:\n",
      "   - Start with high Îµ (explore)\n",
      "   - Gradually decrease (exploit)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploration strategies\n",
    "print(\"\"\"\n",
    "Exploration Strategies:\n",
    "\n",
    "1. Îµ-greedy:\n",
    "   - With prob Îµ: random action\n",
    "   - With prob 1-Îµ: best action\n",
    "   - Simple and effective\n",
    "\n",
    "2. Softmax/Boltzmann:\n",
    "   P(a|s) = exp(Q(s,a)/Ï„) / Î£ exp(Q(s,a')/Ï„)\n",
    "   - Ï„ high â†’ more exploration\n",
    "   - Ï„ low â†’ more exploitation\n",
    "\n",
    "3. Upper Confidence Bound (UCB):\n",
    "   a = argmax[Q(s,a) + câˆš(ln(t)/N(s,a))]\n",
    "   - Explores less-visited actions\n",
    "\n",
    "4. Decaying Îµ:\n",
    "   - Start with high Îµ (explore)\n",
    "   - Gradually decrease (exploit)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd304c7",
   "metadata": {},
   "source": [
    "## 6. Deep Q-Network (DQN)\n",
    "\n",
    "**DQN** (Mnih et al., 2015) menggunakan neural network untuk approximate Q-function.\n",
    "\n",
    "### Key Innovations:\n",
    "1. **Experience Replay**: Store transitions, sample randomly\n",
    "2. **Target Network**: Separate network untuk stable targets\n",
    "3. **CNN for pixels**: End-to-end learning dari raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64fb4f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DQN Architecture:\n",
      "\n",
      "     State s                 Q-values for all actions\n",
      "        â”‚                              â”‚\n",
      "        â–¼                              â”‚\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
      "  â”‚   Input   â”‚                        â”‚\n",
      "  â”‚  (state)  â”‚                        â”‚\n",
      "  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚\n",
      "        â”‚                              â”‚\n",
      "        â–¼                              â”‚\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
      "  â”‚   Dense   â”‚                        â”‚\n",
      "  â”‚   Layers  â”‚                        â”‚\n",
      "  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚\n",
      "        â”‚                              â”‚\n",
      "        â–¼                              â–¼\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "  â”‚  Output: Q(s,aâ‚), Q(s,aâ‚‚), ..., Q(s,aâ‚™) â”‚\n",
      "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Loss = (Q(s,a) - target)Â²\n",
      "Target = r + Î³ max_a' Q_target(s', a')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DQN Architecture\n",
    "print(\"\"\"\n",
    "DQN Architecture:\n",
    "\n",
    "     State s                 Q-values for all actions\n",
    "        â”‚                              â”‚\n",
    "        â–¼                              â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "  â”‚   Input   â”‚                        â”‚\n",
    "  â”‚  (state)  â”‚                        â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "        â”‚                              â”‚\n",
    "        â–¼                              â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "  â”‚   Dense   â”‚                        â”‚\n",
    "  â”‚   Layers  â”‚                        â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "        â”‚                              â”‚\n",
    "        â–¼                              â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚  Output: Q(s,aâ‚), Q(s,aâ‚‚), ..., Q(s,aâ‚™) â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Loss = (Q(s,a) - target)Â²\n",
    "Target = r + Î³ max_a' Q_target(s', a')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad0a9a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer created!\n"
     ]
    }
   ],
   "source": [
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"Replay Buffer created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa7a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent created!\n"
     ]
    }
   ],
   "source": [
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, n_actions, hidden_sizes=[64, 64]):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = self._build_network(hidden_sizes)\n",
    "        self.target_network = self._build_network(hidden_sizes)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.buffer = ReplayBuffer()\n",
    "    \n",
    "    def _build_network(self, hidden_sizes):\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(hidden_sizes[0], activation='relu', input_shape=(self.state_dim,))\n",
    "        ])\n",
    "        for size in hidden_sizes[1:]:\n",
    "            model.add(layers.Dense(size, activation='relu'))\n",
    "        model.add(layers.Dense(self.n_actions))\n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights to target network\"\"\"\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        q_values = self.q_network.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train on batch from replay buffer\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Compute targets\n",
    "        next_q_values = self.target_network.predict(next_states, verbose=0)\n",
    "        max_next_q = np.max(next_q_values, axis=1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "        \n",
    "        # Train\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            # Get Q-values for taken actions\n",
    "            action_masks = tf.one_hot(actions, self.n_actions)\n",
    "            q_values_for_actions = tf.reduce_sum(q_values * action_masks, axis=1)\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.square(targets - q_values_for_actions))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "print(\"DQN Agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5cfdb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DQN Training Loop:\n",
      "\n",
      "for episode in range(num_episodes):\n",
      "    state = env.reset()\n",
      "    \n",
      "    while not done:\n",
      "        # 1. Select action (Îµ-greedy)\n",
      "        action = agent.get_action(state)\n",
      "        \n",
      "        # 2. Take action\n",
      "        next_state, reward, done = env.step(action)\n",
      "        \n",
      "        # 3. Store transition\n",
      "        agent.buffer.add(state, action, reward, next_state, done)\n",
      "        \n",
      "        # 4. Train on batch\n",
      "        agent.train()\n",
      "        \n",
      "        state = next_state\n",
      "    \n",
      "    # 5. Update target network periodically\n",
      "    if episode % update_freq == 0:\n",
      "        agent.update_target_network()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DQN Training loop (conceptual)\n",
    "print(\"\"\"\n",
    "DQN Training Loop:\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # 1. Select action (Îµ-greedy)\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        # 2. Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # 3. Store transition\n",
    "        agent.buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # 4. Train on batch\n",
    "        agent.train()\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # 5. Update target network periodically\n",
    "    if episode % update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626efc1d",
   "metadata": {},
   "source": [
    "### 6.1 DQN Improvements\n",
    "\n",
    "| Improvement | Description |\n",
    "|-------------|-------------|\n",
    "| **Double DQN** | Reduce overestimation bias |\n",
    "| **Dueling DQN** | Separate state value and advantage |\n",
    "| **Prioritized Replay** | Sample important transitions more |\n",
    "| **Noisy Networks** | Learned exploration |\n",
    "| **Rainbow** | Combine all improvements |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f19eccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Double DQN:\n",
      "\n",
      "Problem: Standard DQN overestimates Q-values\n",
      "  target = r + Î³ max_a' Q_target(s', a')\n",
      "  â†‘ Same network selects AND evaluates action\n",
      "\n",
      "Solution: Use different networks for selection and evaluation\n",
      "  a* = argmax_a' Q_online(s', a')   # Online network SELECTS\n",
      "  target = r + Î³ Q_target(s', a*)   # Target network EVALUATES\n",
      "\n",
      "Dueling DQN:\n",
      "\n",
      "Split Q into Value and Advantage:\n",
      "  Q(s, a) = V(s) + A(s, a)\n",
      "  \n",
      "  V(s) = How good is this state?\n",
      "  A(s, a) = How much better is action a than average?\n",
      "\n",
      "Benefits: Better learning when actions don't matter much\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double DQN concept\n",
    "print(\"\"\"\n",
    "Double DQN:\n",
    "\n",
    "Problem: Standard DQN overestimates Q-values\n",
    "  target = r + Î³ max_a' Q_target(s', a')\n",
    "  â†‘ Same network selects AND evaluates action\n",
    "\n",
    "Solution: Use different networks for selection and evaluation\n",
    "  a* = argmax_a' Q_online(s', a')   # Online network SELECTS\n",
    "  target = r + Î³ Q_target(s', a*)   # Target network EVALUATES\n",
    "\n",
    "Dueling DQN:\n",
    "\n",
    "Split Q into Value and Advantage:\n",
    "  Q(s, a) = V(s) + A(s, a)\n",
    "  \n",
    "  V(s) = How good is this state?\n",
    "  A(s, a) = How much better is action a than average?\n",
    "\n",
    "Benefits: Better learning when actions don't matter much\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bedda",
   "metadata": {},
   "source": [
    "## 7. Policy Gradient Methods\n",
    "\n",
    "**Policy Gradient** methods directly optimize the policy.\n",
    "\n",
    "### Key Idea:\n",
    "Instead of learning Q-values â†’ derive policy,\n",
    "directly parameterize policy Ï€_Î¸(a|s) and optimize Î¸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1fafb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value-Based vs Policy Gradient:\n",
      "\n",
      "Value-Based (DQN):\n",
      "  - Learn Q(s, a)\n",
      "  - Derive policy: Ï€(s) = argmax_a Q(s, a)\n",
      "  - Discrete actions only\n",
      "  - Can be unstable\n",
      "\n",
      "Policy Gradient (REINFORCE):\n",
      "  - Directly learn Ï€_Î¸(a|s)\n",
      "  - Continuous actions supported\n",
      "  - Smoother optimization\n",
      "  - High variance\n",
      "\n",
      "Best of both: Actor-Critic methods!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Policy Gradient vs Value-Based\n",
    "print(\"\"\"\n",
    "Value-Based vs Policy Gradient:\n",
    "\n",
    "Value-Based (DQN):\n",
    "  - Learn Q(s, a)\n",
    "  - Derive policy: Ï€(s) = argmax_a Q(s, a)\n",
    "  - Discrete actions only\n",
    "  - Can be unstable\n",
    "\n",
    "Policy Gradient (REINFORCE):\n",
    "  - Directly learn Ï€_Î¸(a|s)\n",
    "  - Continuous actions supported\n",
    "  - Smoother optimization\n",
    "  - High variance\n",
    "\n",
    "Best of both: Actor-Critic methods!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b3bc2",
   "metadata": {},
   "source": [
    "### 7.1 REINFORCE Algorithm\n",
    "\n",
    "**Policy Gradient Theorem:**\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t \\right]\n",
    "$$\n",
    "\n",
    "Dimana $G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ adalah return.\n",
    "\n",
    "**Intuition:**\n",
    "- If action led to high return â†’ increase probability\n",
    "- If action led to low return â†’ decrease probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cacfd738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE Agent created!\n"
     ]
    }
   ],
   "source": [
    "# REINFORCE Agent\n",
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(state_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(n_actions, activation='softmax')  # Action probabilities\n",
    "        ])\n",
    "        \n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        # Episode memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        probs = self.policy.predict(state[np.newaxis], verbose=0)[0]\n",
    "        action = np.random.choice(self.n_actions, p=probs)\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns(self):\n",
    "        \"\"\"Compute discounted returns\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        return returns\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train on episode\"\"\"\n",
    "        returns = self.compute_returns()\n",
    "        states = np.array(self.states)\n",
    "        actions = np.array(self.actions)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get action probabilities\n",
    "            probs = self.policy(states)\n",
    "            action_masks = tf.one_hot(actions, self.n_actions)\n",
    "            selected_probs = tf.reduce_sum(probs * action_masks, axis=1)\n",
    "            \n",
    "            # Policy gradient loss\n",
    "            log_probs = tf.math.log(selected_probs + 1e-8)\n",
    "            loss = -tf.reduce_mean(log_probs * returns)\n",
    "        \n",
    "        grads = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy.trainable_variables))\n",
    "        \n",
    "        # Clear memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "print(\"REINFORCE Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c5302",
   "metadata": {},
   "source": [
    "## 8. Actor-Critic Methods\n",
    "\n",
    "**Actor-Critic** combines policy gradient (Actor) dan value function (Critic).\n",
    "\n",
    "### Components:\n",
    "- **Actor**: Policy Ï€_Î¸(a|s) - decides which action to take\n",
    "- **Critic**: Value function V_w(s) - evaluates how good the state is\n",
    "\n",
    "### Advantage:\n",
    "$$\n",
    "A(s, a) = Q(s, a) - V(s) \\approx r + \\gamma V(s') - V(s)\n",
    "$$\n",
    "\n",
    "Use advantage instead of return to reduce variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2315f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actor-Critic Architecture:\n",
      "\n",
      "        State s\n",
      "           â”‚\n",
      "           â–¼\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚   Shared    â”‚\n",
      "    â”‚   Layers    â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
      "           â”‚\n",
      "     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
      "     â”‚           â”‚\n",
      "     â–¼           â–¼\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  Actor  â”‚ â”‚  Critic â”‚\n",
      "â”‚ Ï€(a|s)  â”‚ â”‚  V(s)   â”‚\n",
      "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
      "     â”‚           â”‚\n",
      "     â–¼           â–¼\n",
      "  Action    State Value\n",
      "  Probs     Estimate\n",
      "\n",
      "Actor loss: -log Ï€(a|s) Ã— Advantage\n",
      "Critic loss: (V(s) - target)Â²\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actor-Critic visualization\n",
    "print(\"\"\"\n",
    "Actor-Critic Architecture:\n",
    "\n",
    "        State s\n",
    "           â”‚\n",
    "           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Shared    â”‚\n",
    "    â”‚   Layers    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "     â”‚           â”‚\n",
    "     â–¼           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Actor  â”‚ â”‚  Critic â”‚\n",
    "â”‚ Ï€(a|s)  â”‚ â”‚  V(s)   â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "     â”‚           â”‚\n",
    "     â–¼           â–¼\n",
    "  Action    State Value\n",
    "  Probs     Estimate\n",
    "\n",
    "Actor loss: -log Ï€(a|s) Ã— Advantage\n",
    "Critic loss: (V(s) - target)Â²\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82a5ed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor-Critic Agent created!\n"
     ]
    }
   ],
   "source": [
    "# Actor-Critic Agent\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Shared base\n",
    "        inputs = keras.Input(shape=(state_dim,))\n",
    "        x = layers.Dense(64, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Actor head\n",
    "        actor_output = layers.Dense(n_actions, activation='softmax')(x)\n",
    "        \n",
    "        # Critic head\n",
    "        critic_output = layers.Dense(1)(x)\n",
    "        \n",
    "        self.model = keras.Model(inputs=inputs, outputs=[actor_output, critic_output])\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        probs, _ = self.model.predict(state[np.newaxis], verbose=0)\n",
    "        action = np.random.choice(self.n_actions, p=probs[0])\n",
    "        return action\n",
    "    \n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            probs, value = self.model(state[np.newaxis])\n",
    "            _, next_value = self.model(next_state[np.newaxis])\n",
    "            \n",
    "            # TD target and advantage\n",
    "            if done:\n",
    "                td_target = reward\n",
    "            else:\n",
    "                td_target = reward + self.gamma * next_value[0, 0]\n",
    "            \n",
    "            advantage = td_target - value[0, 0]\n",
    "            \n",
    "            # Actor loss\n",
    "            action_prob = probs[0, action]\n",
    "            actor_loss = -tf.math.log(action_prob + 1e-8) * advantage\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = tf.square(td_target - value[0, 0])\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        return total_loss.numpy()\n",
    "\n",
    "print(\"Actor-Critic Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763828a",
   "metadata": {},
   "source": [
    "### 8.1 Advanced Actor-Critic Methods\n",
    "\n",
    "| Algorithm | Key Innovation |\n",
    "|-----------|---------------|\n",
    "| **A2C** | Advantage Actor-Critic, synchronous |\n",
    "| **A3C** | Asynchronous, parallel workers |\n",
    "| **PPO** | Clipped objective, stable training |\n",
    "| **SAC** | Soft Actor-Critic, entropy regularization |\n",
    "| **TD3** | Twin Delayed DDPG, continuous actions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c709b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proximal Policy Optimization (PPO):\n",
      "\n",
      "Problem: Large policy updates can be destructive\n",
      "\n",
      "Solution: Clip the policy ratio\n",
      "\n",
      "r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)\n",
      "\n",
      "L_CLIP = min(\n",
      "    r(Î¸) Ã— A,\n",
      "    clip(r(Î¸), 1-Îµ, 1+Îµ) Ã— A\n",
      ")\n",
      "\n",
      "If A > 0: Want to increase action prob, but clip at (1+Îµ)\n",
      "If A < 0: Want to decrease action prob, but clip at (1-Îµ)\n",
      "\n",
      "Benefits:\n",
      "- Simple to implement\n",
      "- Stable training\n",
      "- Good performance\n",
      "- Used in ChatGPT training (RLHF)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PPO concept\n",
    "print(\"\"\"\n",
    "Proximal Policy Optimization (PPO):\n",
    "\n",
    "Problem: Large policy updates can be destructive\n",
    "\n",
    "Solution: Clip the policy ratio\n",
    "\n",
    "r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)\n",
    "\n",
    "L_CLIP = min(\n",
    "    r(Î¸) Ã— A,\n",
    "    clip(r(Î¸), 1-Îµ, 1+Îµ) Ã— A\n",
    ")\n",
    "\n",
    "If A > 0: Want to increase action prob, but clip at (1+Îµ)\n",
    "If A < 0: Want to decrease action prob, but clip at (1-Îµ)\n",
    "\n",
    "Benefits:\n",
    "- Simple to implement\n",
    "- Stable training\n",
    "- Good performance\n",
    "- Used in ChatGPT training (RLHF)!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7e9dd",
   "metadata": {},
   "source": [
    "## 9. Kesimpulan\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **RL Basics:**\n",
    "   - Agent learns through interaction with environment\n",
    "   - Goal: maximize cumulative reward\n",
    "   - Key concepts: state, action, reward, policy\n",
    "\n",
    "2. **Value Functions:**\n",
    "   - V(s): expected return from state s\n",
    "   - Q(s, a): expected return from state s, action a\n",
    "   - Bellman equations for recursive definition\n",
    "\n",
    "3. **Q-Learning:**\n",
    "   - Model-free, off-policy\n",
    "   - Learn Q-values from experience\n",
    "   - Îµ-greedy exploration\n",
    "\n",
    "4. **Deep Q-Network (DQN):**\n",
    "   - Neural network approximates Q-function\n",
    "   - Experience replay for stable training\n",
    "   - Target network for stable targets\n",
    "\n",
    "5. **Policy Gradient:**\n",
    "   - Directly optimize policy parameters\n",
    "   - REINFORCE algorithm\n",
    "   - Works with continuous actions\n",
    "\n",
    "6. **Actor-Critic:**\n",
    "   - Combines value-based and policy-based\n",
    "   - Actor (policy) + Critic (value function)\n",
    "   - PPO is state-of-the-art\n",
    "\n",
    "### Algorithm Selection:\n",
    "\n",
    "| Scenario | Recommended |\n",
    "|----------|-------------|\n",
    "| Discrete actions, small state | Q-Learning |\n",
    "| Discrete actions, large/continuous state | DQN, Rainbow |\n",
    "| Continuous actions | PPO, SAC, TD3 |\n",
    "| Stable training needed | PPO |\n",
    "| Sample efficiency critical | SAC |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a3ceee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                 Reinforcement Learning Summary                   â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                                                                  â”‚\n",
      "â”‚  RL Methods                                                      â”‚\n",
      "â”‚      â”‚                                                           â”‚\n",
      "â”‚      â”œâ”€â”€ Value-Based                                             â”‚\n",
      "â”‚      â”‚       â”œâ”€â”€ Q-Learning (tabular)                           â”‚\n",
      "â”‚      â”‚       â”œâ”€â”€ DQN (deep)                                     â”‚\n",
      "â”‚      â”‚       â””â”€â”€ Double DQN, Dueling DQN, Rainbow               â”‚\n",
      "â”‚      â”‚                                                           â”‚\n",
      "â”‚      â”œâ”€â”€ Policy-Based                                            â”‚\n",
      "â”‚      â”‚       â””â”€â”€ REINFORCE                                       â”‚\n",
      "â”‚      â”‚                                                           â”‚\n",
      "â”‚      â””â”€â”€ Actor-Critic                                            â”‚\n",
      "â”‚              â”œâ”€â”€ A2C, A3C                                        â”‚\n",
      "â”‚              â”œâ”€â”€ PPO (most popular)                              â”‚\n",
      "â”‚              â””â”€â”€ SAC, TD3 (continuous)                           â”‚\n",
      "â”‚                                                                  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  Key Applications:                                               â”‚\n",
      "â”‚  â€¢ Game AI (AlphaGo, Atari, StarCraft)                          â”‚\n",
      "â”‚  â€¢ Robotics (manipulation, locomotion)                          â”‚\n",
      "â”‚  â€¢ LLM Training (RLHF - ChatGPT, Claude)                        â”‚\n",
      "â”‚  â€¢ Autonomous Systems                                            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary visualization\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 Reinforcement Learning Summary                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  RL Methods                                                      â”‚\n",
    "â”‚      â”‚                                                           â”‚\n",
    "â”‚      â”œâ”€â”€ Value-Based                                             â”‚\n",
    "â”‚      â”‚       â”œâ”€â”€ Q-Learning (tabular)                           â”‚\n",
    "â”‚      â”‚       â”œâ”€â”€ DQN (deep)                                     â”‚\n",
    "â”‚      â”‚       â””â”€â”€ Double DQN, Dueling DQN, Rainbow               â”‚\n",
    "â”‚      â”‚                                                           â”‚\n",
    "â”‚      â”œâ”€â”€ Policy-Based                                            â”‚\n",
    "â”‚      â”‚       â””â”€â”€ REINFORCE                                       â”‚\n",
    "â”‚      â”‚                                                           â”‚\n",
    "â”‚      â””â”€â”€ Actor-Critic                                            â”‚\n",
    "â”‚              â”œâ”€â”€ A2C, A3C                                        â”‚\n",
    "â”‚              â”œâ”€â”€ PPO (most popular)                              â”‚\n",
    "â”‚              â””â”€â”€ SAC, TD3 (continuous)                           â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Key Applications:                                               â”‚\n",
    "â”‚  â€¢ Game AI (AlphaGo, Atari, StarCraft)                          â”‚\n",
    "â”‚  â€¢ Robotics (manipulation, locomotion)                          â”‚\n",
    "â”‚  â€¢ LLM Training (RLHF - ChatGPT, Claude)                        â”‚\n",
    "â”‚  â€¢ Autonomous Systems                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
